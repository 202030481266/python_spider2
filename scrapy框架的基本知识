- 什么事框架
    - 就是一个集成了很多功能的并且具有很强通用性的一个项目模板
- 如何学习框架
    - 专门学习框架所对应的框架的详细用法
- scrapy框架
    - 爬虫中的明星框架，功能十分强大
    - 使用：(在终端环境)
        - 创建工程 ： scrapy startproject xxxxx
        - cd xxxxx
        - 在spider的子目录下创建一个爬虫文件 ： scrapy genspider spiderName www.xxx.com（起始url）
        - 执行工程 ： scrapy crawl spiderName (--nolog 可以取消日志输出) 也可以在setting文件中写入LOG_LEVEL = 'ERROR'表示只显示错误日志
    - scrapy数据解析：
        - response 返回对象可以直接使用xpath，但是和etree的xpath有所区别，但用法基本一致
        - xpath解析所得到的是一个列表，但列表元素都是一个selector对象
        - 在项目进行前要确保 UA伪装， 机器人协议改为不遵守 ， 只保留错误日志
        - 可以使用 extract（）方法来提取selector中的 data 参数中的字符串数据，如果对整个xpath返回的列表使用方法则会对列表每一个selector对象进行提取，并且会返回一个列表
        - 在确保返回的列表只有一个元素的前提下 ， 可以使用extract_first()的方法来进行字符串提取
    - 数据的持久化存储
        - 基于终端的存储：只能用于parse方法返回值存储到本地文本文件中（但有文件格式限制）
            scrapy crawl spiderName -o filepath
        - 基于管道的存储：
            - 编码流程：
                - 数据解析
                - 在item类中定义相关的属性
                - 将解析的数据存储到item类型的对象中
                - 将item类型对象交给管道进行持久化存储操作
                - 在管道类的process_item中要将其接受到的item对象中存储的数据进行持久化存储
                - 在setting中开启管道
                - 一个管道类对应一个数据存储的平台和媒体
- 基于spider的全站数据爬取
    - 就是将网站的某板块的下的全部页码对应的页面进行爬取和解析
    - 需求：爬取校花网的图片名称
        - 实现方式：
            - 将每一页的对应的url添加到start_urls中
            - 手动发送请求（yield spider.Request(url, callback)）callback是专门用来进行数据解析的方法
- scrapy五大核心组件：
    - 引擎：用来处理整个系统的数据流，触发事务（框架核心）
    - 调度器：用来接受引擎发过来的请求，压入队列中，并在引擎中再次请求时返回，可以想象成一个URL的优先队列，由他来决定下一个要抓取的是什么，同时去除重复的网址
    - 下载器：用于下载网页内容，并将网页内容返回给蜘蛛（scrapy下载器是建立在twisted这个高效的异步模型上的）
    - 爬虫：爬虫是主要干活的，用于从特定网页中提取自己需要的信息，即所谓的实体(item)。用户也可以从中提取出链接，让scrapy继续抓取下一个页面
    - 项目管道：负责处理爬虫从网页中抽取的实体（item），主要的功能是持久化实体，验证实体的有效性，清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
- 请求传参
    - 使用场景：爬取的数据不在同一张页面。（深度爬取）
    - 需求：爬取BOSS的岗位名称，岗位描述
    - 传递item类型对象（使用meta）
- 图片数据爬取之ImagePipeline
    - 基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
        - 字符串：只需要基于xpath进行解析且提交管道进行持久化存储
        - 图片：xpath解析出图片src属性，单独的对相应的url请求图片二进制数据
    - ImagePipeline：
        - 只需要将img的src属性进行解析提交到管道，管道就会对图片的src请求发送获取图片的数据，且还会进行持久化存储
        - 使用流程：
            - 解析图片url
            - 将存储的图片地址的item提交到制订的管道类
            - 在管道文件自定制一个基于imagespipline的一个管道类：
                - from scrapy.pipeline.images import ImagesPipeline
                - get_media_request  (使用item中的链接获得响应数据)
                - file_path (指定图片名)
                - item_completed （表示完成）
            - 在配置文件中操作：
                - 指定图片的存储的路径：IMAGES_STORE = './imgs'
                - 指定开启的管道类：自定制的管道
- 中间件：（个人认为下载中间件很像fidder）
    - 下载中间件：在引擎和下载器之间
        - 批量拦截整个工程中所有的请求和响应
        - 拦截请求：进行UA伪装， 设置代理IP，
        - 拦截响应： 篡改响应数据和对象
            - 需求：爬取网页新闻的新闻数据（对应的标题和内容）
                - 1.解析出五大板块的url（通过抓包工具可以知道板块的url是静态的）
                - 2.每一个板块的新闻标题都是动态加载出来的
                - 3，通过解析每一个详情页的请求的得到的内容，得到相应的内容
    - 爬虫中间件：在spider和引擎之间
- CrawlSpider(一个类，是Spider的子类)
    - 基于Spider：手动请求
    - 基于CrawlSpider
        - 创建一个文件 ：scrapy genspider -t crawl name www.xxx.com
        - 链接解析器：
            - 根据指定规则（allow正则模式）获取链接
        - 规则解析器：
            - 根据链接解析器提取的链接进行规则解析，指定相对应的回调函数